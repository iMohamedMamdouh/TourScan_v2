{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup & Data Preparation**"
      ],
      "metadata": {
        "id": "shvrUmxIA9kw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiH3VdDRAnbf",
        "outputId": "14f79fac-7522-4374-ca64-f7b92f75b010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1MBtxjgIjSickaraV4GKOkHEM9jfHwWcu\n",
            "From (redirected): https://drive.google.com/uc?id=1MBtxjgIjSickaraV4GKOkHEM9jfHwWcu&confirm=t&uuid=11ec5074-826d-4527-bfff-0c087d1c2a87\n",
            "To: /content/archive.zip\n",
            "100%|██████████| 5.29G/5.29G [00:56<00:00, 93.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset downloaded and extracted!\n",
            "✅ Descriptions loaded!\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "drive_url = \"https://drive.google.com/uc?id=1MBtxjgIjSickaraV4GKOkHEM9jfHwWcu\"\n",
        "zip_path = \"/content/archive.zip\"\n",
        "extract_path = \"/content/dataset\"\n",
        "\n",
        "gdown.download(drive_url, zip_path, quiet=False)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"✅ Dataset downloaded and extracted!\")\n",
        "\n",
        "xlsx_path = os.path.join(extract_path, \"ardata set.xlsx\")\n",
        "df = pd.read_excel(xlsx_path)\n",
        "\n",
        "monument_descriptions = dict(zip(df[\"exhibits\"], df[\"Text in English\"]))\n",
        "\n",
        "print(\"✅ Descriptions loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "JNHDt9EvBLjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    os.path.join(extract_path, \"training_set\"),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    os.path.join(extract_path, \"training_set\"),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\"\n",
        ")\n",
        "\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "print(\"✅ Data Preprocessing Done!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-7mVzEMBRXw",
        "outputId": "c0e48dd2-d915-401e-8a37-d34933d89e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3441 images belonging to 72 classes.\n",
            "Found 843 images belonging to 72 classes.\n",
            "✅ Data Preprocessing Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Pretrained Model & Modify for Training**"
      ],
      "metadata": {
        "id": "7fBRmCzFDs8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\")\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "for layer in base_model.layers[:100]:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(256, activation=\"relu\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(128, activation=\"relu\")(x)\n",
        "x = Dense(len(class_names), activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "print(\"✅ Model Ready for Training!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dve3vVciDu5X",
        "outputId": "f031b28d-4320-4b4a-a3b6-366eb3de9b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "✅ Model Ready for Training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Model & Save Every 5 Epochs**"
      ],
      "metadata": {
        "id": "50AZ2zC0Dxz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/trained_model\"\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=f\"{save_path}_epoch{{epoch:02d}}.h5\",\n",
        "    save_freq=5 * len(train_generator),\n",
        "    save_best_only=True,\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"max\",\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "print(\"✅ Google Drive mounted and checkpoint callback ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZoUqkexDzno",
        "outputId": "be99ef4a-6c93-48b3-93b6-6441c3791d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Google Drive mounted and checkpoint callback ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train the Model with Checkpoints**"
      ],
      "metadata": {
        "id": "hpDw42-SD41P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_images(folder_path):\n",
        "    total = 0\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tif', '.tiff')):\n",
        "                total += 1\n",
        "    return total\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/archive/training_set'\n",
        "test_dir = '/content/drive/MyDrive/archive/test_set'\n",
        "\n",
        "train_count = count_images(train_dir)\n",
        "test_count = count_images(test_dir)\n",
        "\n",
        "print(f\"Training sample: {train_count}\")\n",
        "print(f\"Test samples: {test_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3zwA2A_KLdt",
        "outputId": "467feee9-2c8d-4ba1-e931-80df0c6e41cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sample: 9822\n",
            "Test samples: 511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    callbacks=[checkpoint_callback]\n",
        ")\n",
        "\n",
        "print(\"✅ Training Completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPnVz65gD6wA",
        "outputId": "03724d66-4566-4d2b-e957-3a3f8f539a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 5s/step - accuracy: 0.1583 - loss: 3.7530 - val_accuracy: 0.3535 - val_loss: 3.1413\n",
            "Epoch 2/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 5s/step - accuracy: 0.4895 - loss: 2.1775 - val_accuracy: 0.5018 - val_loss: 2.1771\n",
            "Epoch 3/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 4s/step - accuracy: 0.6435 - loss: 1.4739 - val_accuracy: 0.5409 - val_loss: 1.8687\n",
            "Epoch 4/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 5s/step - accuracy: 0.7339 - loss: 1.0577 - val_accuracy: 0.6204 - val_loss: 1.5605\n",
            "Epoch 5/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.7791 - loss: 0.8818"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/model_checkpoint.py:202: UserWarning: Can save best model only with val_accuracy available, skipping.\n",
            "  self._save_model(epoch=self._current_epoch, batch=batch, logs=logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 5s/step - accuracy: 0.7794 - loss: 0.8809 - val_accuracy: 0.6928 - val_loss: 1.1618\n",
            "Epoch 6/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 4s/step - accuracy: 0.8514 - loss: 0.5980 - val_accuracy: 0.7746 - val_loss: 0.8858\n",
            "Epoch 7/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m478s\u001b[0m 4s/step - accuracy: 0.8957 - loss: 0.4322 - val_accuracy: 0.8031 - val_loss: 0.6950\n",
            "Epoch 8/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 4s/step - accuracy: 0.9230 - loss: 0.3046 - val_accuracy: 0.7936 - val_loss: 0.7260\n",
            "Epoch 9/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 5s/step - accuracy: 0.9275 - loss: 0.2607 - val_accuracy: 0.8019 - val_loss: 0.7096\n",
            "Epoch 10/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 5s/step - accuracy: 0.9468 - loss: 0.1982 - val_accuracy: 0.8458 - val_loss: 0.5309\n",
            "Epoch 11/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 4s/step - accuracy: 0.9582 - loss: 0.1592 - val_accuracy: 0.8826 - val_loss: 0.4749\n",
            "Epoch 12/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m484s\u001b[0m 4s/step - accuracy: 0.9587 - loss: 0.1408 - val_accuracy: 0.8743 - val_loss: 0.4922\n",
            "Epoch 13/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 5s/step - accuracy: 0.9724 - loss: 0.1118 - val_accuracy: 0.8814 - val_loss: 0.4739\n",
            "Epoch 14/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m510s\u001b[0m 5s/step - accuracy: 0.9709 - loss: 0.1001 - val_accuracy: 0.8565 - val_loss: 0.5878\n",
            "Epoch 15/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 5s/step - accuracy: 0.9756 - loss: 0.0808 - val_accuracy: 0.9063 - val_loss: 0.3903\n",
            "Epoch 16/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 5s/step - accuracy: 0.9754 - loss: 0.0840 - val_accuracy: 0.9205 - val_loss: 0.3313\n",
            "Epoch 17/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 5s/step - accuracy: 0.9748 - loss: 0.0855 - val_accuracy: 0.8897 - val_loss: 0.4474\n",
            "Epoch 18/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 4s/step - accuracy: 0.9852 - loss: 0.0618 - val_accuracy: 0.9205 - val_loss: 0.3577\n",
            "Epoch 19/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 5s/step - accuracy: 0.9864 - loss: 0.0495 - val_accuracy: 0.8778 - val_loss: 0.5331\n",
            "Epoch 20/20\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 5s/step - accuracy: 0.9852 - loss: 0.0545 - val_accuracy: 0.9193 - val_loss: 0.3652\n",
            "✅ Training Completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save The fine tuned model**"
      ],
      "metadata": {
        "id": "vWixAfv8fklV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/trained_model\"\n",
        "\n",
        "model.save(f\"{save_path}.h5\")\n",
        "print(\"✅ Model saved successfully!\")\n",
        "\n",
        "class_map_path = f\"{save_path}_classes.json\"\n",
        "with open(class_map_path, \"w\") as f:\n",
        "    json.dump(class_names, f)\n",
        "print(\"✅ Class names saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_DTashdfmGU",
        "outputId": "93f7fe73-b779-4246-c8c3-035f52c65f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Model saved successfully!\n",
            "✅ Class names saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the model**"
      ],
      "metadata": {
        "id": "5n7kkgQ-foEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import gdown\n",
        "\n",
        "model_url = \"https://drive.google.com/uc?id=1aO1YqsqEryhAFt61RnZxLTV0ESQPMN1_\"\n",
        "json_url = \"https://drive.google.com/uc?id=10VPVoewDB9ZhqxAOYpA1w8sW8UX8QZQH\"\n",
        "xlsx_url = \"https://drive.google.com/uc?id=1He3KtMi4R4X2Sjs66YaMUIeXpHzL4aSr\"\n",
        "\n",
        "model_path = \"/content/monument_model.h5\"\n",
        "json_path = \"/content/trained_model.json\"\n",
        "xlsx_path = \"/content/monuments.xlsx\"\n",
        "\n",
        "gdown.download(model_url, model_path, quiet=False)\n",
        "gdown.download(json_url, json_path, quiet=False)\n",
        "gdown.download(xlsx_url, xlsx_path, quiet=False)\n",
        "\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "df = pd.read_excel(xlsx_path)\n",
        "monument_descriptions = dict(zip(df.iloc[:, 0], df.iloc[:, 1]))\n",
        "\n",
        "print(\"✅ Model loaded successfully!\")\n",
        "print(\"✅ Monument descriptions loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "t020kTsdfnwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Make Predictions & Retrieve Descriptions**"
      ],
      "metadata": {
        "id": "7w5LKiMLfrUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "class_names_path = \"/content/trained_model.json\"\n",
        "with open(class_names_path, \"r\") as f:\n",
        "    class_names = json.load(f)\n",
        "\n",
        "print(\"✅ Class Names Loaded:\", class_names)\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "def predict_monument(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_index = np.argmax(predictions)\n",
        "    predicted_class = class_names[predicted_index]\n",
        "\n",
        "    description = monument_descriptions.get(predicted_class, \"No description available.\")\n",
        "\n",
        "    return predicted_class, description\n",
        "\n",
        "print(\"📤 Please upload an image for prediction:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "image_path = list(uploaded.keys())[0]\n",
        "print(f\"📸 Image received: {image_path}\")\n",
        "\n",
        "monument, desc = predict_monument(image_path)\n",
        "\n",
        "print(f\"🔹 Predicted Monument: {monument}\")\n",
        "print(f\"📜 Information: {desc}\")\n"
      ],
      "metadata": {
        "id": "zpg9BnRoftNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the Model**"
      ],
      "metadata": {
        "id": "6vya64oOf1DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.models.load_model(\"monument_model.h5\")\n"
      ],
      "metadata": {
        "id": "kOnR67xQf2o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert the Model to TFLite**"
      ],
      "metadata": {
        "id": "er-xvs7gf4RM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"✅ Model successfully converted to TensorFlow Lite!\")\n"
      ],
      "metadata": {
        "id": "8qu_ydtPf6uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimize the TFLite Model**"
      ],
      "metadata": {
        "id": "u0TeDG9-f8ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quantized_model = converter.convert()\n",
        "\n",
        "with open(\"model_quantized.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_quantized_model)\n",
        "\n",
        "print(\"✅ Quantized TensorFlow Lite model saved!\")\n"
      ],
      "metadata": {
        "id": "lHas92Cof997"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Upload the Converted Model to Google Drive**"
      ],
      "metadata": {
        "id": "AGf1M-AtgAVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mv model.tflite /content/drive/MyDrive/\n",
        "print(\"✅ Model uploaded to Google Drive!\")"
      ],
      "metadata": {
        "id": "7kGR_aIPgBjc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}